# 2025/12/30

## Accomplishments

- Meninjau kriteria yang harus dipenuhi untuk submission kelas Dicoding Deep Learning Tingkat Mahir. 

- Merumuskan pertanyaan pemandu buat pelajari hal yang diperlukan untuk memenuhi Kriteria 1 submission-nya:
  1. Data leakage dalam konteks machine learning / deep learning itu apa?
  2. Kenapa normalisasi data ataupun standardisasi data dalam konteks machine learning time forecasting itu berpotensi bikin terjadinya data leakage?
  3.  Gimana cara mastiin data leakage tidak terjadi saat melakukan normalisasi atau standardisasi pada data?
  4.  Baseline model LSTM itu sampai sebatas gimana baseline yang bener-bener baseline?
  5.  Analisis dekomposisi itu untuk apa?
  6.  Gimana cara melakukan analisis dekomposisi?
  7.  Kenapa yang diminta adalah analisis dekomposisi pada data target? Kenapa ga untuk data fitur juga?
  8.  Pelajari lagi tentang window size (dari kelas fundamental deep learning)
  9.  Analisis lag itu apa?
  10.  Uji ACF dan uji PACF itu analisis lag? Gunanya untuk menentukan window size?
  11.  Gimana cara melakukan uji ACF dan uji PACF?
  12.  Gimana caranya bikin fitur baru pakai Rolling Statistic? Seperti apa bentuknya?
 
- Baca modul dan dapat jawaban dari pertanyaan rumusan i sampai iii:
  - Alasan dari kenapa normalisasi dan standardisasi ga cocok untuk memperbaiki skala data untuk keperluan time series forecasting, adalah karena hasil skala dari kedua metode tersebut jadi bikin tren hilang, jadi bikin ngilangin pola penting karena datanya jadi "rata".
  - Selain itu, alasan lainnya kenapa normalisasi dan standardisasi ga cocok dipakai untuk penskalaan data time series (jika, katakan, diterapkan sebelum membagi dataset menjadi data train dan data test) adalah, karena kedua metode tersebut menggunakan statistik deskriptif (mean, std) dari keseluruhan data. Dan karena menggunakan itu, alhasil datanya jadi ngintip. Ngintipnya pas melakukan perhitungan si mean dari seluruh data tadi. Dan, jadinya bocor deh (yang mana dinamakan sebagai data leakage).
  - Solusi dari keterbatasan normalisasi dan standardisasi untuk penskalaan data time series forecasting: percentage change transformation. Jadi dari yang tadinya datanya itu dalam bentuk nilai konkrit sebenarnya kayak tanggal segini harga jual 5000, tanggal besoknya harga jualnya 6000, setelah pakai penskalaan data pakai percentage change transformation, datanya sekarang jadi dalam bentuk, tanggal segini terjadi kenaikan/penurunan sebesar sekian persen, tanggal besoknya sekian persen, dst. Dan kekuatan dari persentase itu, dia sifatnya "universal", yang mana sejalan dengan tujuan penskalaan data dengan normalisasi / standardisasi. Terlebih, metode ini dijamin ga bikin data leakage karena rumus perhitungannya itu cukup melibatkan data tanggal hari h dengan data tanggal hari setelahnya (h+1). Jadi, ga ada lagi deh yang namanya ga sengaja ngintip data dari data masa depan. 

## Thoughts
- Nge-list pertanyaan secara mandiri kayak gini bikin ngerjain submission jadi terarah dan ga hilang arah
- Kira-kira bisa selesai berapa hari ya ngerjain submission-nya?

## Next Steps
- Belajar belajar belajar. Cari dan pelajari jawaban dari sisa semua pertanyaan yang udah kurumuskan tadi. 
