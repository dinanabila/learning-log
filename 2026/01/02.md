# 2026/01/02

## Accomplishments
- Mencari dan mempelajari jawaban dari pertanyaan pemandu untuk ngerjain Kriteria 2 submission kelas Dicoding Deep Learning Tingkat Mahir yang udah dibuat kemarin:
    1. Jelasin tentang:
        - Sequence to sequence LSTM
            - Sequence to sequence LSTM itu beda sama LSTM biasa. Kalau LSTM biasa, dia ga memprediksi seluruh data sekaligus dalam satu waktu, melainkan, ngeprediksinya untuk setiap waktu. Dan oleh karenanya, data yang dipakai LSTM untuk memprediksi juga selalu data aktual, ga pakai data hasil prediksi di waktu sebelumnya. 
            - Nah kalau sequence to sequence sebaliknya. Dia juga memprediksi dengan mempertimbangkan data hasil prediksi di waktu sebelumnya. Sebutan modelnya: Seq2Seq LSTM. 
        - Teacher Forcing
            - Teacher Forcing itu salah satu strategi pelatihan yang dapat diterapkan untuk membangun model LSTM Seq2Seq. Ibaratnya, anggaplah si model kita ini si murid. Dengan Teacher Forcing, si murid ini diajari dengan cara seperti guru mengajari murid dengan cara menyuapi jawabannya langsung tanpa membuat si murid mencari tahu jawabannya sendiri secara mandiri. Tujuannya biar proses belajar tetap aman terkendali.
            - Contoh teknisnya, kalau tanpa teacher forcing, si murid alias si model ini bakal ngeprediksi hari +2 pakai hasil prediksi hari+1, lalu hari +3 pakai hasil prediksi hari+2. Jadi kek dilepas gitu aja tanpa dituntun guru. Akibatnya apa. Kalau dia salah prediksi satu aja, misal, di hasil prediksi hari+2 hasil error kek mae nya tinggi banget, imbasnya jadi nular ke hasil prediksi untuk hari-hari setelahnya. 
            - Nah kalau pakai teacher forcing, si model bakal ngeprediksi pakai jawaban asli dari setiap langkah sebagai inputnya. Jadi kayak ngasih contekan yang bener. 
        - Functional API
            - Kalau yang biasanya dipakai untuk belajar di kelas pemula itu, namanya sequential API. Contohnya: 
                ```
                # sequential api

                from tensorflow.keras import Sequential
                from tensorflow.keras.layers import Dense
                
                seq_model = Sequential([
                    Dense(128, activation='relu'),
                    Dense(64, activation='relu'),
                    Dense(1, activation='sigmoid')
                    ])
                ```
            - dan kalau contoh di atas diadaptasi jadi dalam bentuk Functional API, jadinya gini:
                ```
                # functional api

                from tensorflow.keras import Model
                from tensorflow.keras.layers import Dense, Input
                
                #Inisiasi input layer
                input_layer = Input(shape=(4,))
                
                #Susun layer yang terhubung hingga ke output layer
                dense_1 = Dense(128, activation='relu')(input_layer)
                dense_2 = Dense(64, activation='relu')(dense_1)
                output_layer = Dense(1, activation='sigmoid')(dense_2)
                
                #Deklarasikan model dengan input dan output
                model = Model(inputs=input_layer, outputs=output_layer)
                ```
            - Terus bedanya di mana? Di tingkat kefleksibelannya. Kalau Functional API, dia itu antara layernya itu bisa disambung-sambungin dengan fleksibel / modular, ga kayak Sequential API yang saklek harus berurutan antar layer yang udah dideklarasikan. Liat deh, itu contohnya aja keliatan dari susunan layer kalau pakai functional api. Fleksibel gitu dia ibarat kalau fungsi di matematika itu f(g(h(x))), terus bisa disesuaiin jadi f(h(g(x))), atau bahkan bisa juga g(f(x)) aja. Caranya, dengan bikin layer-layer output yang dipengenin, terus tarok di parameter outputs nya Model(). 
            - Dari situ kita tau kalau functional api itu, dia input nya bisa lebih dari satu, outputnya juga bisa lebih dari satu. Tuh liat aja parameter dari Model(). Namanya inputs kan, bukan input? Terus juga outputs, bukan output. 
        - Custom Layer
            - Meskipun Tensorflow udah nyediain layer bawaan buat langsung kita comot dan pakai, di lapangannya, boleh jadi justru kita perlu layer yang lebih spesifik tujuan penggunaannya. Dan itu, bisa kita wujudkan dengan membuat custom layer. 
        
        - Multistep time series forecasting
            - Multistep di sini itu nyambung sama penjelasan tentang bedanya LSTM biasa dengan LSTM sequence to sequence. LSTM sequence to sequence itu mendukung multistep. Multistep, di mana si model memprediksi semua data yang mau diprediksi secara sekaligus sebanyak sejumlah step (yang mana sesuai namanya, jelas multi lebih dari satu), tergantung berapa step yang ditentukan di awalnya. Step di sini sama dengan horizon, sepenangkapanku sekarang. 

        - Model Subclassing
            - Ini versi lebih fleksibelnya lagi dari Functional API. Konsepnya beda-beda tipis sama custom layer. Dengan menerapkan model subclassing, kita jadi bisa bangun arsitektur model yang lebih custom lagi, jauh lebih custom dari yang ditawarkan Functional API. 

        - Multi-Head Attention
            - Attention itu ibarat pelengkap, yang bikin model bisa lebih fokus ke hal yang lebih penting, alias, bagian data yang paling relevan. Harapannya dengan menambahkan ini, model jadi bisa lebih adaptif terhadap dinamika data. 
    
    2. Gimana cara membangun model Seq2Seq Teacher Forcing menggunakan Model Subclassing?
        - Caranya dengan membuat class baru yang mewarisi kelas Model dari Tensorflow. Lalu bangun strukturnya yang terdiri dari init dan call. Lebih detailnya ada di submodul Arsitektur Kompleks Functional API vs Model Class. 

    3. Gimana cara membuat ulang layer dari nol dengan Custom Layer? 
        - Caranya dengan membuat class baru yang mewarisi kelas Layer dari Tensorflow. Lalu class baru itu kita isi dengan fungsi-fungsi sesuai dengan kebutuhan layer yang kita inginkan. Untuk detail caranya ada di submodul Kustomisasi Layer dengan Subclassing. 

- Sempet baca juga kalau ternyata percentage change transformation itu kurang tepat buat multivariate time series forecasting, soalnya kalau multivariate itu kan dia pakai lebih dari satu fitur, dan antar fitur nya itu kemungkinan besar ada lah sedikitnya saling mempengaruhinya. Alhasil kalau kita pakai percentage change transformation, pola hubungan relatifnya jadi bisa bias. Jadi untuk multivariate time series  buat ngerjain Kriteria 1, mending pakai normalisasi / standardisasi setelah datanya di-split jadi data train-test-val. Ingat, setelah ya, jangan sebelum, biar datanya ga ngintip. 

## Thoughts
- Jadi ini output kerjaan dari Kriteria 2 itu, adalah arsitektur model LSTM Seq2Seq yang mengadopsi strategi Teacher Forcing dan pastiin dia memenuhi multi-step time series forecasting sebanyak 24 step. 
- Kalau mau mencapai kriteria Basic, cukup bangun pakai Functional API aja udah memenuhi. Bikin juga satu layer Dense dari nol, pakai Custom Layer. 
- Kalau mau mencapai kriteria Skilled, ga hanya pakai Functional API, tapi, tunjukkan juga kalau model dibangun menggunakan Model Subclassing. Terus, lengkapi juga arsitekturnya dengan menambahkan layer Multi-Head Attention di baseline model LSTM dari Kriteria 1, sama ke model Seq2Seq LSTM nya. 
- Kalau mau mencapai Advanced, itu si layer Multi-Head Attention nya dibangun ulang dari nol pakai Custom Layer, terus terapin ke arsitektur baseline sama seq2seq modelnya. Tambahin juga satu custom layer lainnya, bebas, contoh yang bisa dibangun: Dropout layer, Normalization layer, Activation function layer (Elu, Leaky Relu, dll).


## Next Steps
- Merumuskan pertanyaan pemandu untuk ngerjain Kriteria 3. 
