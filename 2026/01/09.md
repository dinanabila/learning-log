# 2026/01/09

## Accomplishments
- Bangun model seq2seq lstm pakai functional api
- Bangun model pakai subclassing
- Bangun custom loss MAE weighted
- Bangun custom callback early stopping
- Bangun custom callback learning rate reduced
- Bangun custom training
- Run loop training
- Debugging custom training dan model

## Thoughts
- Awalnya itu kan aku pakai 500 window, soalnya berdasarkan hasil uji acf, lag nya itu masih bagus di 500an. Window 500 mungkin memang gede, tapi kupikir gapapa karena kan jumlah row datasetnya juga banyak, 50ribuan. Tapi ternyata pas modelnya dilatih, LAMA BANGET. Dan jadi overfitting parah. 
- Jadi kukurangi window nya jadi 111 aja. Terus batch_size juga kukurangi dari ribuan, jadi 100an aja karena nge-crash kalau ribuan.
- Sejauh ini aman walau masih overfitting. Yang penting proses training nya ga selama yang awal. 
- kwargs itu penting. Tadinya kuanggap sepele jadi sempet ga kupake pas bangun custom layer dkk. TAPI TERNYATA PENTING. Pokoknya kalau mau nge-inherit class lain tapi ragu bakal ada yang ketinggal dideklarasiin, pakai kwargs aja biar pasti dan ga ada yang ketinggal lagi. 

## Next Steps
- Inference.
