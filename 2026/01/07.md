# 2026/01/07

## Accomplishments
- Baca tentang self-attention dan multi-head attention dari The Illustrated Transformer - Jay Alammar: https://github.com/dinanabila/nlp-reads/blob/main/2026/07012026%20-%20The%20Illustrated%20Transformer%20%E2%80%93%20Jay%20Alammar%20%E2%80%93%20Visual.pdf

## Thoughts
- Algoritma untuk rakit ulang multi-head attention untuk custom layer udah kebayang. Besok tinggal eksekusi. 

## Next Steps
- Bangun custom layer Multi-head Attention
