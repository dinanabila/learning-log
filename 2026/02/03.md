# 2026/02/03

## Accomplishments
- Mencerna kembali instruksi submission dltm.
- Garis besarnya: 
    - Bangun custom layer: dense, multi-head attention, activation function.
    - Bangun baseline model LSTM direct multi-step forecasting (24 step), pakai custom layer yang udah dibangun.
    - Bangun model LSTM seq2seq multistep (24 step) yang menerapkan teacher forcing, pakai custom layer yang udah dibangun.
    - Bangun custom loss dan custom callback early stopping.
    - Bangun custom training, dan pakai custom loss dan callback yang udah dibangun. 
    - Train model pakai custom training yang udah dibangun.
    - Evaluasi model.
    - Bangun inference untuk lstm biasa dan lstm seq2seq.
    - Inference pakai data test. Visualisasikan hasilnya pakai line chart yang memperlihatkan perbandingan antara hasil prediksi dengan data aktual target. 

## Thoughts
- Pertanyaan yang muncul di pikiran:
    - Kenapa cuma seq2seq yang inferensinya pakai autoregressive? Kenapa baseline ngga?
        - Karena kita mau lihat bedanya antara yang pakai autoregressive dengan yang tidak.
    - Selain autoregressive, ada lagi ga perbedaan antara lstm biasa dengan lstm seq2seq?
    - LSTM seq2seq harus banget pakai teacher forcing? Atau, cuma salah satu opsi di antara opsi-opsi lainnya?
    - Bagian apa saja dari arsitektur model lstm seq2seq yang bisa benar-benar di-custom, dan bagian mana yang memang dari sananya ga boleh diubah?
    - Serupa seperti pertanyaan sebelumnya, bagian mana dari arsitektur inference lstm seq2seq yang boleh dimodifikasi?
    - Kan pas training itu model lstm seq2seq nya pakai teacher forcing yang pakai ground truth data aktual. Terus memangnya masih ada unsur yang bikin si model lstm seq2seq itu bener-bener beda sama direct multistep? Soalnya kan secara esensi, kayak sama gitu antara lstm seq2seq teacher forcing dengan direct multistep.
    - Kenapa hidden state selalu bergantung pada step sebelumnya? Apa yang berubah di hidden state? Apa yang bikin berubah?

## Next Steps
- Pending time series forecasting lagi. Perlu fokus di eksplorasi pre-trained model YOLO dan LLM Hugging Face.